{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import TfidfModel\n",
    "import numpy as np\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"training_data/training_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:11<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_train_vecs = pd.read_csv(\"vecs/bert_train_vecs.csv\")\n",
    "bert_train_vecs = bert_train_vecs.values.tolist()\n",
    "\n",
    "bert_test_docs = test_data.allegation_desc\n",
    "bert_test_docs = json.loads(bert_test_docs.to_json(orient='records'))\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "bert_test_vecs = sentence_model.encode(bert_test_docs, show_progress_bar=True)\n",
    "bert_test_vecs = np.array(bert_test_vecs)\n",
    "bert_test_vecs = pd.DataFrame(bert_test_vecs).to_csv(\"vecs/bert_test_vecs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_train_vecs = pd.read_csv(\"vecs/gensim_train_vecs.csv\")\n",
    "gensim_train_vecs = gensim_train_vecs.values.tolist()\n",
    "\n",
    "gensim_model = gensim.models.ldamodel.LdaModel.load(\"models/gensim_train.model\")\n",
    "\n",
    "gensim_test_docs = test_data.allegation_desc\n",
    "\n",
    "def lemmatization(descs, allowed_pos_tags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    final_text = []\n",
    "    for desc in descs:\n",
    "        doc = nlp(desc)\n",
    "        new_text = \" \".join([token.lemma_ for token in doc if token.pos_ in allowed_pos_tags])\n",
    "        final_text.append(new_text)\n",
    "    return (final_text)\n",
    "\n",
    "lemmatized_texts = lemmatization(gensim_test_docs)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return list(bigram[doc] for doc in texts)\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return list(trigram[bigram[doc]] for doc in texts)\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "test_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tdidf = TfidfModel(test_corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tdif = []\n",
    "\n",
    "for i in range(0, len(test_corpus)):\n",
    "    bow = test_corpus[i]\n",
    "    low_value_words = []\n",
    "    tdif_ids = [id for id, value in tdidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tdidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tdif\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tdif = [id for id in bow_ids if id not in tdif_ids]\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tdif]\n",
    "    test_corpus[i] = new_bow\n",
    "\n",
    "def get_test_vecs():\n",
    "    gensim_test_vecs = []\n",
    "    for i in range(len(gensim_test_docs)):\n",
    "        top_topics = gensim_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "        topic_vec.extend([len(gensim_test_docs.iloc[i])]) # length review\n",
    "        gensim_test_vecs.append(topic_vec)\n",
    "    return gensim_test_vecs\n",
    "\n",
    "\n",
    "gensim_test_vecs = get_test_vecs()\n",
    "gensim_test_vecs = np.array(gensim_test_vecs)\n",
    "gensim_test_vecs = pd.DataFrame(gensim_test_vecs).to_csv(\"vecs/gensim_test_vecs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 11:05:22,558 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-02-02 11:05:22,613 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-02-02 11:05:23,703 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-02-02 11:05:38,835 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-02-02 11:05:38,858 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "t2v_train_model = Top2Vec.load(\"models/top2vec_train_model\")\n",
    "t2v_train_vecs = t2v_train_model.document_vectors\n",
    "\n",
    "def convert_test_data_to_list(data):\n",
    "    t2v_test_docs = [x for x in data[\"allegation_desc\"]]\n",
    "    return t2v_test_docs\n",
    "\n",
    "t2v_test_docs = convert_test_data_to_list(test_data)\n",
    "t2v_test_model = Top2Vec(t2v_test_docs, embedding_model_path=\"models/top2vec_train_model\")\n",
    "t2v_test_vecs = t2v_test_model.document_vectors\n",
    "\n",
    "t2v_test_vecs = np.array(t2v_test_vecs)\n",
    "t2v_test_vecs = pd.DataFrame(t2v_test_vecs).to_csv(\"vecs/t2v_test_vecs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
