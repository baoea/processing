{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numba\\np\\arraymath.py:3845: DeprecationWarning: `np.MachAr` is deprecated (NumPy 1.22).\n",
      "  @overload(np.MachAr)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "from top2vec import Top2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipynb\n",
    "# from ipynb.fs.full.topic_models import split_data, lda_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"training_data/training_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179\n",
       "1    171\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### lda ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train_vecs = pd.read_csv(\"vecs/lda_vecs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel.load(\"models/lda_train.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = test_data.allegation_desc\n",
    "\n",
    "def lemmatization(descs, allowed_pos_tags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    final_text = []\n",
    "    for desc in descs:\n",
    "        doc = nlp(desc)\n",
    "        new_text = \" \".join([token.lemma_ for token in doc if token.pos_ in allowed_pos_tags])\n",
    "        final_text.append(new_text)\n",
    "    return (final_text)\n",
    "\n",
    "lemmatized_texts = lemmatization(test_docs)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return list(bigram[doc] for doc in texts)\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return list(trigram[bigram[doc]] for doc in texts)\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "test_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tdidf = TfidfModel(test_corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tdif = []\n",
    "\n",
    "for i in range(0, len(test_corpus)):\n",
    "    bow = test_corpus[i]\n",
    "    low_value_words = []\n",
    "    tdif_ids = [id for id, value in tdidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tdidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tdif\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tdif = [id for id in bow_ids if id not in tdif_ids]\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tdif]\n",
    "    test_corpus[i] = new_bow\n",
    "\n",
    "def get_test_vecs():\n",
    "    lda_test_vecs = []\n",
    "    for i in range(len(test_docs)):\n",
    "        top_topics = lda_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "        topic_vec.extend([len(test_docs.iloc[i])]) # length review\n",
    "        lda_test_vecs.append(topic_vec)\n",
    "    return lda_test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_test_vecs = get_test_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### top2vec ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2_vec_model_train = Top2Vec.load(\"models/noso\")\n",
    "top_train_vecs = top_2_vec_model_train.document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test_data_to_list(data):\n",
    "    test_docs = [x for x in data[\"allegation_desc\"]]\n",
    "    return test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:30:47,910 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-01-31 15:30:47,951 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-01-31 15:30:48,737 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-01-31 15:30:53,249 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-01-31 15:30:53,259 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "test_docs = convert_test_data_to_list(test_data)\n",
    "\n",
    "top_2_vec_model_test = Top2Vec(test_docs, embedding_model_path=\"models/noso\")\n",
    "\n",
    "top_test_vecs = top_2_vec_model_test.document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################# regress #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(top_test_vecs)\n",
    "y_test = np.array(test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test = np.pad(X_test, pad_width=((0,350),(0,0)), mode='constant')\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(top_train_vecs)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.621 +- 0.013\n",
      "Logisitic Regression SGD Val f1: 0.579 +- 0.006\n",
      "SVM Huber Val f1: 0.367 +- 0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=48)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True,\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"lr_scores\"] = lr.predict(scaler.transform(X_test))\n",
    "    test_data[\"lr_scores_prob_1\"] = lr.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "\n",
    "    y_pred = lr.predict(scaler.transform(X_val_scale))\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"sgd_scores\"] = sgd.predict(scaler.transform(X_test))\n",
    "    test_data[\"sgd_scores_prob_1\"] = sgd.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "\n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"sgd_huber_scores\"] = sgd_huber.predict(scaler.transform(X_test))\n",
    "    test_data[\"sgd_huber_scores_prob_1\"] = sgd_huber.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>allegation_desc</th>\n",
       "      <th>meta_agency</th>\n",
       "      <th>meta_tracking_id</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>lr_scores</th>\n",
       "      <th>lr_scores_prob_1</th>\n",
       "      <th>sgd_scores</th>\n",
       "      <th>sgd_scores_prob_1</th>\n",
       "      <th>sgd_huber_scores</th>\n",
       "      <th>sgd_huber_scores_prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>613</td>\n",
       "      <td>16272</td>\n",
       "      <td>complainant documented the accused officers di...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2016-0706-p</td>\n",
       "      <td>inadequate description</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0</td>\n",
       "      <td>3.577752e-113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>715</td>\n",
       "      <td>16740</td>\n",
       "      <td>complaint: complainant stated police came to a...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2017-0385-r</td>\n",
       "      <td>evidence</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913350</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>13924</td>\n",
       "      <td>reported after scheduled time for duty.</td>\n",
       "      <td>new-orleans-so</td>\n",
       "      <td>D-015-19</td>\n",
       "      <td>internal misconduct/administrative infractions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0</td>\n",
       "      <td>4.174592e-47</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962</td>\n",
       "      <td>17848</td>\n",
       "      <td>officers were accused of not documenting evide...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2020-0078-r</td>\n",
       "      <td>evidence</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724844</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>808</td>\n",
       "      <td>17155</td>\n",
       "      <td>complainant accused officer of failing to veri...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2018-0236-p</td>\n",
       "      <td>evidence</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999789</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>565</td>\n",
       "      <td>16048</td>\n",
       "      <td>complainant stated that she was falsely arrest...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2016-0462.p</td>\n",
       "      <td>arrests/stops/searches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "      <td>6.674609e-156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>666</td>\n",
       "      <td>16507</td>\n",
       "      <td>complainant stated that an unknown nopd office...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2017-0128-p</td>\n",
       "      <td>traffic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0</td>\n",
       "      <td>2.041634e-140</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>629</td>\n",
       "      <td>16344</td>\n",
       "      <td>the officer illegally search a subject’s backp...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2016-0786.r</td>\n",
       "      <td>arrests/stops/searches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405793</td>\n",
       "      <td>0</td>\n",
       "      <td>2.376089e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>355</td>\n",
       "      <td>15103</td>\n",
       "      <td>officer allegedly screamed and yelled at the c...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2014-0771-c</td>\n",
       "      <td>harm to the general public safety</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991661</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>282</td>\n",
       "      <td>14778</td>\n",
       "      <td>complainant alleged the officer did not do his...</td>\n",
       "      <td>new-orleans-pd</td>\n",
       "      <td>2014-0242-c</td>\n",
       "      <td>juvenile</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930579</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     id                                    allegation_desc  \\\n",
       "0           613  16272  complainant documented the accused officers di...   \n",
       "1           715  16740  complaint: complainant stated police came to a...   \n",
       "2            93  13924            reported after scheduled time for duty.   \n",
       "3           962  17848  officers were accused of not documenting evide...   \n",
       "4           808  17155  complainant accused officer of failing to veri...   \n",
       "..          ...    ...                                                ...   \n",
       "345         565  16048  complainant stated that she was falsely arrest...   \n",
       "346         666  16507  complainant stated that an unknown nopd office...   \n",
       "347         629  16344  the officer illegally search a subject’s backp...   \n",
       "348         355  15103  officer allegedly screamed and yelled at the c...   \n",
       "349         282  14778  complainant alleged the officer did not do his...   \n",
       "\n",
       "        meta_agency meta_tracking_id  \\\n",
       "0    new-orleans-pd      2016-0706-p   \n",
       "1    new-orleans-pd      2017-0385-r   \n",
       "2    new-orleans-so         D-015-19   \n",
       "3    new-orleans-pd      2020-0078-r   \n",
       "4    new-orleans-pd      2018-0236-p   \n",
       "..              ...              ...   \n",
       "345  new-orleans-pd      2016-0462.p   \n",
       "346  new-orleans-pd      2017-0128-p   \n",
       "347  new-orleans-pd      2016-0786.r   \n",
       "348  new-orleans-pd      2014-0771-c   \n",
       "349  new-orleans-pd      2014-0242-c   \n",
       "\n",
       "                                              label  target  lr_scores  \\\n",
       "0                            inadequate description       0          0   \n",
       "1                                          evidence       0          1   \n",
       "2    internal misconduct/administrative infractions       1          0   \n",
       "3                                          evidence       0          1   \n",
       "4                                          evidence       0          1   \n",
       "..                                              ...     ...        ...   \n",
       "345                          arrests/stops/searches       0          0   \n",
       "346                                         traffic       0          0   \n",
       "347                          arrests/stops/searches       0          0   \n",
       "348               harm to the general public safety       1          1   \n",
       "349                                        juvenile       0          1   \n",
       "\n",
       "     lr_scores_prob_1  sgd_scores  sgd_scores_prob_1  sgd_huber_scores  \\\n",
       "0            0.000062           0      3.577752e-113                 0   \n",
       "1            0.913350           1       1.000000e+00                 0   \n",
       "2            0.010896           0       4.174592e-47                 0   \n",
       "3            0.724844           1       1.000000e+00                 0   \n",
       "4            0.999789           1       1.000000e+00                 0   \n",
       "..                ...         ...                ...               ...   \n",
       "345          0.000006           0      6.674609e-156                 0   \n",
       "346          0.000017           0      2.041634e-140                 0   \n",
       "347          0.405793           0       2.376089e-08                 0   \n",
       "348          0.991661           1       1.000000e+00                 0   \n",
       "349          0.930579           1       1.000000e+00                 0   \n",
       "\n",
       "     sgd_huber_scores_prob_1  \n",
       "0                   0.250132  \n",
       "1                   0.251412  \n",
       "2                   0.246417  \n",
       "3                   0.250613  \n",
       "4                   0.263790  \n",
       "..                       ...  \n",
       "345                 0.265821  \n",
       "346                 0.265987  \n",
       "347                 0.195111  \n",
       "348                 0.264261  \n",
       "349                 0.197760  \n",
       "\n",
       "[350 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_pred = test_data[~((test_data.target == \"0\"))]\n",
    "# test_pred\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
